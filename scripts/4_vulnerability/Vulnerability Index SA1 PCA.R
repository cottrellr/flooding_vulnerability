### ------------------------------------------------------------------------ ###
### ------------- CODE FOR CREATION OF VULNERABILITY INDEX SA1 ------------- ###
### ------------------------------------------------------------------------ ###


### Libraries: ------------------------------------------------------------- ###
### ------------------------------------------------------------------------ ###
vi_libraries <- c('sf', 'tidyverse', 'dplyr', 
                  'here', 'data.table', 'corrplot', 
                  'ggplot2', 'tidyr', 'corrr', 
                  'ggcorrplot', 'factoextra', 'gridExtra')
sapply(vi_libraries, require, character.only = TRUE)

### SHAPEFILE LOADING: ----------------------------------------------------- ###
### ------------------------------------------------------------------------ ###
setwd("~/Flooding/flooding_vulnerability/scripts/4_vulnerability")
BRIS <- read_sf("../../data/spatial/SA1_Bris","bris_sa1")
DF <- st_drop_geometry(BRIS)[,1]
fPath <- "../../data/processed_data/"
fPath2 <- "../../data/data_products/"
df <- read_sf("../../data/SA1_2021_AUST_SHP_GDA2020/SA1_2021_AUST_GDA2020.shp")

### NORMALISING FUNCTION: -------------------------------------------------- ###
### ------------------------------------------------------------------------ ###
normaliseData <- function(dummy){
  for (i in 2:ncol(dummy)){
    # Avoid modifying columns if they have a single unique value or are NA
    if(length(unique(dummy[, i])) > 1){
      dummy[, i] <- scale(dummy[, i])
    }
  }
  return(dummy)
}

### EXPOSURE (HIGH -> MORE VULNERABLE): ------------------------------------ ###
### ------------------------------------------------------------------------ ###
DF_E <- DF

# Inundation:
DATASET_E <- c("sa1_inundation")
for (i in 1:length(DATASET_E)){
  new <- read.csv(paste0(fPath2,DATASET_E[i],".csv"))
  new <- new[,c(1, ncol(new))]
  names(new)[1] <- "SA1_CODE21"
  DF_E <- merge(DF_E, new)
}

# Normalise
DF_E <- normaliseData(DF_E)

if(ncol(DF_E) == 2){
  # Quick counter-measure while inundation is our only exposure parameter
  DF_E <- cbind(DF_E, "EXP" = DF_E[,2])
}

# Plot Exposure (Revisit to adjust score representation)
PLOT <- merge(BRIS,DF_E)
ggplot() +
  geom_sf(data=PLOT, aes(fill = EXP), color=NA) +
  theme_void() +
  scale_fill_distiller(direction = 1) +
  labs(fill="Exposure Score")

# Save the plot inside PCA folder
ggsave("../../visualisations/PCA/Exposure_Score.png")


### SENSITIVITY (HIGH -> MORE VULNERABLE): --------------------------------- ###
### ------------------------------------------------------------------------ ###
DF_S <- DF

# Set sensitivity themes & variables within themes
S_THEMES <- list(
  # Socioeconomic Status (missing income status)
  soc_econ_stat = c("pop_density2021", # Population density = population/area (in km2)
                    "edu2021",         # % completed high school (15 years and above)
                    "unemployed2021"), # % Unemployment for age 15+
  # House and Household Composition
  house_comp = c("vulnerable_young2021", # % infant (<5)
                 "vulnerable_old2021",   # % old (70+)
                 "disabl2021",           # % Disability (cannot do core activities without assistance)
                 "dwell_house2021",      # % Dwellings less than 3 stories
                 "familydwelling2021"),  # % single parent families and more than one family in a household
  # Health Status, Language, and Culture
  health_culture = c("esl2021",                  # % English as second language
                     "indigenous2021",            # % indigenous
                     "long-term_health_cond2021") # % living with long-term health conditions
)

# ABS Demographics:
DATASET_S <- c("disabl2021",
               "dwell_house2021",
               "edu2021",
               "vulnerable_young2021",
               "vulnerable_old2021",
               "unemployed2021",
               "esl2021",
               "indigenous2021",
               "long-term_health_cond2021",
               "Population2021",
               "familydwelling2021")

# Read the files for each of the sensitivity elements
for (i in 1:length(DATASET_S)){
  new <- read.csv(paste0(fPath, "SA1/", DATASET_S[i], ".csv"))
  new <- new[, c(1, ncol(new))]
  names(new) <- c("SA1_CODE21", DATASET_S[i])
  DF_S <- merge(DF_S, new)
}    

# Task 3: Calculate population density = pop/area or people per km2 - DONE
merged_DF_S <- merge(df[, c("SA1_CODE21", "AREASQKM21")], 
                     DF_S, 
                     by = "SA1_CODE21", 
                     all.y = TRUE)
DF_S <- DF_S %>%
  mutate(pop_density2021 = merged_DF_S$Population2021/merged_DF_S$AREASQKM21,
         edu_vuln2021 = 1 - DF_S$edu2021) %>%
  select(-c(Population2021, edu2021))
S_THEMES$soc_econ_stat <- c("pop_density2021",
                            "edu_vuln2021",
                            "unemployed2021")

DF_S_long <- pivot_longer(DF_S, cols = -1)
hist(DF_S$vulnerable_old2021)
ggplot(filter(DF_S_long, name == "vulnerable_old2021",
              value > 0), aes(x = value)) + 
  geom_histogram(bins = 75) +
  facet_wrap(~name, scales = "free") +
  theme_minimal() +
  labs(x = "Value", y = "Frequency")
summary(DF_S$vulnerable_old2021)

test <- merge(BRIS, 
              filter(DF_S_long, 
                     name == "vulnerable_old2021",
                     value < 1))
ggplot(test) +
  geom_sf(color = NA, aes(fill = value)) +
  xlim(c(152.8,153)) +
  ylim(c(-28,-27))

# Set any outliers more than 2 std from mean to min or max depending
for(column in 2:ncol(DF_S)){
  # Determine outliers
  lower_bound <- quantile(DF_S[, column], 0.025, na.rm = T)
  upper_bound <- quantile(DF_S[, column], 0.975, na.rm = T)
  outlier_ind_low <- which(DF_S[, column] < lower_bound)
  outlier_ind_high <- which(DF_S[, column] > upper_bound)
  
  # Set any outliers more than 2 std from mean to min or max depending
  if (length(outlier_ind_low) > 0) {
    DF_S[outlier_ind_low, column] <- min(DF_S[-outlier_ind_low, column], 
                                         na.rm = T)
  }
  
  if (length(outlier_ind_high) > 0) {
    DF_S[outlier_ind_high, column] <- max(DF_S[-outlier_ind_high, column], 
                                          na.rm = T)
  }
}

# Normalise the data
DF_S_colnames <- colnames(DF_S)
DF_S <- normaliseData(DF_S)
colnames(DF_S) <- DF_S_colnames

# Produce correlation plot
corr_matrix <- cor(DF_S[, 2:ncol(DF_S)],
                   use = "na.or.complete")
ggcorrplot(corr_matrix, 
           lab = F, 
           title = "Correlation Matrix of Sensitivity Variables",
           outline.col = "white",
           tl.cex = 10,
           lab_size = 3,
           ggtheme = theme_minimal(),
           show.diag = TRUE,
           type = "upper",
           legend.title = "Correlation Coefficient")

# Extract the upper triangle of the correlation matrix without the diagonal
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat, diag = TRUE)] <- NA
  return(cormat)
}

upper_tri <- get_upper_tri(corr_matrix)

# Melt the correlation matrix and filter strong correlations
melted_corr <- reshape2::melt(upper_tri, na.rm = TRUE)

# Function to create interpretation sentences
interpret_correlations <- function(row) {
  correlation_value <- as.numeric(row["value"])
  if (correlation_value > 0.5) {
    direction <- "strong positive"
  } else if (correlation_value > 0) {
    direction <- "weak positive"
  } else if (correlation_value < -0.5) {
    direction <- "strong negative"
  } else {
    direction <- "weak negative"
  }
  paste(row["Var1"], row["Var2"], ":", direction, "correlation")
}

# Apply the function to each row of the filtered correlations
interpretations <- apply(melted_corr, 1, interpret_correlations)

# Output the interpretations
for (sentence in interpretations) {
  cat(sentence, "\n")
}

# PCA overall
data.pca <- princomp(corr_matrix)
summary(data.pca)
fviz_eig(data.pca, addlabels = TRUE) #scree

# Plot histograms
DF_S_long <- pivot_longer(DF_S, cols = -1)
ggplot(DF_S_long, aes(x = value)) + 
  geom_histogram(bins = 75) +
  facet_wrap(~name, scales = "free") +
  theme_minimal() +
  labs(x = "Value", y = "Frequency")
summary(DF_S)

# PCA for each theme
# Initialise lists to store results
s_theme_results <- list()
for (s_theme_name in names(S_THEMES)) {
  # Initialise a list for this theme
  results <- list()
  s_theme <- S_THEMES[[s_theme_name]]
  
  if (length(s_theme) > 1) {
    # Correlation plot of the theme
    theme_corr_matrix <- cor(DF_S[, s_theme],
                             use = "na.or.complete")
    ggcorrplot(theme_corr_matrix)
    results$corr_matrix <- theme_corr_matrix
    results$corr_plot <- ggcorrplot(theme_corr_matrix)
    
    # PCA of the theme
    theme_data.pca <- princomp(theme_corr_matrix)
    results$pca_summary <- summary(theme_data.pca)
    results$pca_loadings <- theme_data.pca$loadings
    
    # Scree plot
    results$scree_plot <- fviz_eig(theme_data.pca, addlabels = TRUE)
    
    # PCA variables plot
    results$pca_var_plot <- fviz_pca_var(theme_data.pca, col.var = "black")
  }
  
  # Store all results for this theme in the main list
  s_theme_results[[s_theme_name]] <- results
}
s_theme_results

# For Comp columns in pca_loadings that sum below 0, multiply by -1
# Iterate over each theme's PCA results
for (s_theme_name in names(s_theme_results)) {
  if (!is.null(s_theme_results[[s_theme_name]]$pca_loadings)) {
    # Extract the loadings
    loadings <- s_theme_results[[s_theme_name]]$pca_loadings
    # Check the sum of each component's loadings
    for (comp in 1:ncol(loadings)) {
      if (sum(loadings[, comp]) < 0) {
        # Multiply the loadings by -1 if the sum is below 0
        loadings[, comp] <- loadings[, comp] * -1
      }
    }
    # Update the loadings in the results
    s_theme_results[[s_theme_name]]$pca_loadings <- loadings
    # Print adjusted loadings for verification
    print(paste("Adjusted loadings for theme:", s_theme_name))
    print(s_theme_results[[s_theme_name]]$pca_loadings)
  }
}

# Multiply PC1 % of explained variance by Comp columns that are 
# useful (85% var explained) and sum to get overall index weights
# Function to calculate the overall index weights
calculate_index_weights <- function(pca_results) {
  # Get the explained variance
  explained_variance <- pca_results$sdev^2 / sum(pca_results$sdev^2)
  # Determine the number of components explaining at least 85% variance
  cumulative_variance <- cumsum(explained_variance)
  useful_components <- which(cumulative_variance <= 0.85)
  # Ensure at least one component is selected if none are below 85%
  if (length(useful_components) == 0) {
    useful_components <- 1
  }
  # Get the loadings of the useful components
  loadings <- pca_results$loadings[, useful_components, drop = FALSE]
  # Multiply the PC1 percentage of explained variance by the loadings
  pc1_variance <- explained_variance[1]
  index_weights <- rowSums(loadings * pc1_variance)
  return(index_weights)
}

# Iterate over each theme's PCA results to calculate index weights
for (s_theme_name in names(s_theme_results)) {
  if (!is.null(s_theme_results[[s_theme_name]]$pca_summary)) {
    pca_results <- s_theme_results[[s_theme_name]]$pca_summary
    index_weights <- calculate_index_weights(pca_results)
    s_theme_results[[s_theme_name]]$index_weights <- index_weights
  }
}

# Print the index weights for verification
for (s_theme_name in names(s_theme_results)) {
  if (!is.null(s_theme_results[[s_theme_name]]$index_weights)) {
    print(paste("Index weights for theme:", s_theme_name))
    print(s_theme_results[[s_theme_name]]$index_weights)
  }
}

# Function to calculate and plot the index weights
plot_index_weights <- function(theme_name, index_weights) {
  # Convert the index weights to a dataframe for plotting
  weights_df <- data.frame(
    Variable = names(index_weights),
    Weight = index_weights
  )
  # Create a bar plot of the index weights
  ggplot(weights_df, 
         aes(x = reorder(Variable, Weight), y = Weight)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    coord_flip() + # Flip coordinates for better readability
    theme_minimal() +
    labs(
      title = paste("Index Weights for Theme:", 
                    theme_name),
      x = "Variable",
      y = "Weight"
    )
}

# Iterate over each theme's index weights and plot them
for (s_theme_name in names(s_theme_results)) {
  if (!is.null(s_theme_results[[s_theme_name]]$index_weights)) {
    index_weights <- s_theme_results[[s_theme_name]]$index_weights
    print(plot_index_weights(s_theme_name, 
                             index_weights))
  }
}

# PCA within themes
# Function to impute missing values with column mean
impute_missing_values <- function(data) {
  data %>% mutate(across(everything(), 
                         ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))
}

# Perform PCA within each theme and store the PC1 scores
for (s_theme_name in names(S_THEMES)) {
  s_theme <- S_THEMES[[s_theme_name]]
  if (length(s_theme) > 1) {
    theme_data <- DF_S[, s_theme, drop = FALSE]
    theme_data <- impute_missing_values(theme_data) # Impute missing values
    theme_data_normalised <- normaliseData(theme_data) # Normalise data
    # Perform PCA
    pca_theme <- princomp(theme_data_normalised, 
                          cor = TRUE)
    # Store PC1 scores
    DF_S[[paste0(s_theme_name, "_PC1")]] <- pca_theme$scores[, 1]
    # Store results for reference
    s_theme_results[[s_theme_name]]$pca_scores <- pca_theme$scores[, 1]
    s_theme_results[[s_theme_name]]$pca_summary <- summary(pca_theme)
    s_theme_results[[s_theme_name]]$pca_loadings <- pca_theme$loadings
    s_theme_results[[s_theme_name]]$scree_plot <- fviz_eig(pca_theme, 
                                                           addlabels = TRUE)
    s_theme_results[[s_theme_name]]$pca_var_plot <- fviz_pca_var(pca_theme,
                                                                 col.var = "black")
  }
}

# Collect all the pca_var_plots into a list
pca_var_plots <- lapply(names(S_THEMES), function(s_theme_name) {
  if (!is.null(s_theme_results[[s_theme_name]]$pca_var_plot)) {
    return(s_theme_results[[s_theme_name]]$pca_var_plot)
  } else {
    return(NULL)
  }
})

# Remove NULL values from the list
pca_var_plots <- pca_var_plots[!sapply(pca_var_plots, is.null)]

# Arrange all PCA variable plots side by side
do.call("grid.arrange", 
        c(pca_var_plots, 
          ncol = length(pca_var_plots)))

# PCA across themes
# Combine the PC1 scores from each theme into a new dataframe
DF_combined_themes <- DF_S %>%
  select(SA1_CODE21, ends_with("_PC1")) %>%
  drop_na() # Ensure there are no NA values

# Normalise the combined PC1 scores
DF_combined_themes_normalised <- normaliseData(DF_combined_themes)

# Perform PCA on the normalised combined PC1 scores (excluding SA1_CODE21)
pca_combined_themes <- princomp(DF_combined_themes_normalised[, -1], cor = TRUE)

# Summary of the PCA
summary(pca_combined_themes)

# Scree plot to visualise explained variance
fviz_eig(pca_combined_themes, addlabels = TRUE)

# Plot the PCA results
fviz_pca_var(pca_combined_themes, col.var = "black")

# Correlation plot of the combined PC1 scores
corr_matrix_combined_themes <- cor(DF_combined_themes_normalised[, -1], 
                                   use = "complete.obs")
ggcorrplot(corr_matrix_combined_themes)

# Print the loadings for interpretation
print(pca_combined_themes$loadings)

# Store PCA results in the list
s_theme_results[["combined_themes"]] <- list(
  pca_summary = summary(pca_combined_themes),
  pca_loadings = pca_combined_themes$loadings,
  scree_plot = fviz_eig(pca_combined_themes, addlabels = TRUE),
  pca_var_plot = fviz_pca_var(pca_combined_themes, col.var = "black"),
  corr_plot = ggcorrplot(corr_matrix_combined_themes)
)

# Calculate overall sensitivity of each area from the theme weights
# Combine theme PC1 scores into a new dataframe
DF_sensitivity <- DF_S %>%
  select(SA1_CODE21, ends_with("_PC1"))

# Check if all themes have PC1 scores
missing_themes <- setdiff(names(S_THEMES), 
                          gsub("_PC1", "", 
                               colnames(DF_sensitivity)[-1]))
if (length(missing_themes) > 0) {
  stop(paste("Missing PC1 scores for themes:", 
             paste(missing_themes, 
                   collapse = ", ")))
}

# Calculate overall sensitivity score
DF_sensitivity <- DF_sensitivity %>%
  rowwise() %>%
  mutate(Overall_Sensitivity = sum(c_across(ends_with("_PC1")))) %>%
  ungroup()

# Normalise the overall sensitivity score to the range [0, 1]
min_sensitivity <- min(DF_sensitivity$Overall_Sensitivity)
max_sensitivity <- max(DF_sensitivity$Overall_Sensitivity)
DF_sensitivity <- DF_sensitivity %>%
  mutate(Normalised_Sensitivity = (Overall_Sensitivity - min_sensitivity) / 
           (max_sensitivity - min_sensitivity))

# View the resulting dataframe
head(DF_sensitivity)

# Plot normalised overall sensitivity
ggplot(DF_sensitivity, 
       aes(x = Normalised_Sensitivity)) +
  geom_histogram(fill = "steelblue") +
  theme_minimal() +
  labs(
    title = "Distribution of Normalised Sensitivity",
    x = "Normalised Sensitivity Score",
    y = "Frequency"
  ) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Plot distribution of sensitivity scores
PLOT <- merge(BRIS, 
              DF_sensitivity)
ggplot() +
  geom_sf(data = PLOT, 
          aes(fill = Normalised_Sensitivity), 
          color=NA) +
  scale_fill_distiller(palette = "YlOrBr", direction = 1) +
  theme_void() +
  labs(fill="Sensitivity (%)")

# Save the plot inside PCA folder
ggsave("../../visualisations/PCA/Sensitivity_Score.png")

# Plot correlation of within & across theme sensitivity with overall
corr_matrix <- cor(cbind(DF_S[, 2:ncol(DF_S)],
                         PLOT$Normalised_Sensitivity),
                   use = "na.or.complete")
ggcorrplot(corr_matrix)

# Check: Some are negatively correlated within themes
# Function to calculate and plot correlation matrix for a theme
plot_theme_correlations <- function(theme_name, theme_vars, data) {
  # Calculate the correlation matrix for the theme
  theme_data <- data[, theme_vars, drop = FALSE]
  corr_matrix <- cor(theme_data, use = "na.or.complete")
  
  # Identify negative correlations
  neg_corr <- corr_matrix[corr_matrix < 0]
  
  # Print negative correlations for the theme
  if (length(neg_corr) > 0) {
    cat(paste("Negative correlations in theme:", theme_name, "\n"))
    print(neg_corr)
  }
  
  # Plot the correlation matrix
  ggcorrplot(corr_matrix, 
             lab = TRUE, 
             title = theme_name,
             outline.col = "white",
             tl.cex = 10,
             lab_size = 3,
             ggtheme = theme_minimal()) +
    labs(x = "Variables", y = "Variables") +
    theme(legend.position = "none")
}

# Loop through each theme and plot the correlation matrix
for (theme_name in names(S_THEMES)) {
  theme_vars <- S_THEMES[[theme_name]]
  plot <- plot_theme_correlations(theme_name, theme_vars, DF_S)
  print(plot)
}

# Show all the plots side by side
library(ggpubr)
ggarrange(plot_theme_correlations("Socioeconomic Status", 
                                  S_THEMES$soc_econ_stat, 
                                  DF_S),
          plot_theme_correlations("Health Status, Language,\nand Culture", 
                                  S_THEMES$health_culture, 
                                  DF_S),
          plot_theme_correlations("House and Household\nComposition", 
                                  S_THEMES$house_comp, 
                                  DF_S),
          # PCA between themes correlation matrix with values
          ggcorrplot(cor(DF_combined_themes_normalised[, -1], 
                         use = "complete.obs"),
                     lab = TRUE, 
                     title = "PCA Between Themes",
                     outline.col = "white",
                     tl.cex = 10,
                     lab_size = 3,
                     ggtheme = theme_minimal()) +
            labs(x = "Themes", y = "Themes") +
            theme(legend.position = "none"),
          ncol = 2,
          nrow = 2,
          common.legend = TRUE,
          legend = "right")

### ADAPTIVE CAPACITY (HIGH -> MORE VULNERABLE): --------------------------- ###
### ------------------------------------------------------------------------ ###
DF_A <- DF

# Distance to services:
DATASET_A <- c("dAmbu",
               "dBuss",
               "dFire",
               "dHosp",
               "dPets",
               "dPoli",
               "dSesf",
               "dShop")

# Read the files for each of the sensitivity elements
for (i in 1:length(DATASET_A)){
  new <- read.csv(paste0(fPath,"SA1/",DATASET_A[i],".csv"))[,-1]
  DF_A <- merge(DF_A,new)
}

# Normalise the data
DF_A_colnames <- colnames(DF_A)
DF_A <- normaliseData(DF_A)
colnames(DF_A) <- DF_A_colnames

# Plot histograms
DF_A_long <- pivot_longer(DF_A, cols = -1)
ggplot(DF_A_long, aes(x = value)) + 
  geom_histogram(bins = 75) +
  facet_wrap(~name, scales = "free") +
  theme_minimal() +
  labs(x = "Value", y = "Frequency")
summary(DF_A)

# PCA

# Correlation plot of the theme
ac_corr_matrix <- cor(DF_A[,-1],
                         use = "na.or.complete")
ggcorrplot(ac_corr_matrix)
results$corr_matrix <- ac_corr_matrix
results$corr_plot <- ggcorrplot(ac_corr_matrix)

# PCA of AC
ac_data.pca <- princomp(DF_A[,-1])

# Extract PC1 scores from the PCA
DF_A$AC_PC1 <- ac_data.pca$scores[, 1]

# Normalize the PC1 scores
min_ac <- min(DF_A$AC_PC1, na.rm = TRUE)
max_ac <- max(DF_A$AC_PC1, na.rm = TRUE)
DF_A$AC_Normalised <- (DF_A$AC_PC1 - min_ac) / (max_ac - min_ac)
DF_A$AC_Normalised <- 1-DF_A$AC_Normalised

# Check the distribution of normalized AC scores
hist(DF_A$AC_Normalised, breaks = 75, 
     main = "Distribution of Normalised Adaptive Capacity", 
     xlab = "Normalised AC Score")

# Plot distribution of adaptive capacity scores
PLOT <- merge(BRIS, DF_A)
ggplot() +
  geom_sf(data = PLOT, 
          aes(fill = AC_Normalised), 
          color = NA) +
  scale_fill_distiller(palette = 'YlGn', direction = 1) +
  theme_void() +
  labs(fill="Adaptive Capacity (%)")

# Save the plot inside PCA folder
ggsave("../../visualisations/PCA/Adaptive_Capacity_Score.png")

### VULNERABILITY (HIGH -> MORE VULNERABLE): ------------------------------- ###
### ------------------------------------------------------------------------ ###

# Merge Adaptive Capacity with Exposure and Sensitivity data
DF_final <- DF_sensitivity %>%
  select(SA1_CODE21, Normalised_Sensitivity) %>%
  merge(DF_E %>% select(SA1_CODE21, EXP), by = "SA1_CODE21") %>%
  merge(DF_A %>% select(SA1_CODE21, AC_Normalised), by = "SA1_CODE21")

# Combine the normalised Sensitivity, Exposure, and Adaptive Capacity scores
pca_input <- DF_final %>%
  select(SA1_CODE21, Normalised_Sensitivity, EXP, AC_Normalised) %>%
  drop_na() # Ensure no missing values

############# OPTION 1: PCA on the combined data ############################
# Perform PCA on the combined data
pca_result <- prcomp(pca_input[, -1], scale. = TRUE)

# Summary of the PCA to check the explained variance
summary(pca_result)

# Scree plot to visualize the variance explained by each component
fviz_eig(pca_result, addlabels = TRUE)

# Extract the PC1 loadings (weights)
pc1_loadings <- pca_result$rotation[, 1]

# Multiply the original normalised scores by their corresponding PC1 loadings
DF_final <- DF_final %>%
  mutate(Vulnerability_Index_PCA_Weighted = 
           Normalised_Sensitivity * pc1_loadings["Normalised_Sensitivity"] +
           EXP * pc1_loadings["EXP"] -
           AC_Normalised * pc1_loadings["AC_Normalised"])

# Normalise the Vulnerability Index PCA weighted
min_vi_pca <- min(DF_final$Vulnerability_Index_PCA_Weighted, na.rm = TRUE)
max_vi_pca <- max(DF_final$Vulnerability_Index_PCA_Weighted, na.rm = TRUE)
DF_final <- DF_final %>%
  mutate(Vulnerability_Index_PCA_Normalised = 
           (Vulnerability_Index_PCA_Weighted - min_vi_pca) / (max_vi_pca - min_vi_pca))

# Plot histogram of the PCA-weighted vulnerability index
ggplot(DF_final, aes(x = Vulnerability_Index_PCA_Normalised)) +
  geom_histogram(fill = "steelblue", bins = 75) +
  theme_minimal() +
  labs(title = "Distribution of PCA-Weighted Vulnerability Index (3 PCA Layers)",
       x = "Normalised Vulnerability Index",
       y = "Frequency")

# Plot spatial distribution
PLOT_pca <- merge(BRIS, DF_final, by = "SA1_CODE21")
ggplot() +
  geom_sf(data = PLOT_pca, 
          aes(fill = Vulnerability_Index_PCA_Normalised), 
          color = NA) +
  scale_fill_viridis_c(option = "rocket", direction = -1, begin = 0.2) +
  theme_void() +
  labs(fill = "Vulnerability Index (3x PCA Weighted)")

# Save the final vulnerability map
ggsave("../../visualisations/PCA/Vulnerability_Index_3xPCA.png")

############# OPTION 2: Rank the combined data ##################

# Rank the normalised Sensitivity, Exposure, and Adaptive Capacity scores
DF_final <- DF_final %>%
  mutate(Vulnerability_Index_Rank = 
           rowSums(apply(DF_final[, -1], 2, rank)))

# Normalise the ranked vulnerability index
min_vi_rank <- min(DF_final$Vulnerability_Index_Rank, na.rm = TRUE)
max_vi_rank <- max(DF_final$Vulnerability_Index_Rank, na.rm = TRUE)
DF_final <- DF_final %>%
  mutate(Vulnerability_Index_Rank_Normalised = 
           (Vulnerability_Index_Rank - min_vi_rank) / (max_vi_rank - min_vi_rank))

# Plot histogram of the ranked vulnerability index
ggplot(DF_final, aes(x = Vulnerability_Index_Rank_Normalised)) +
  geom_histogram(fill = "steelblue", bins = 75) +
  theme_minimal() +
  labs(title = "Distribution of Ranked Vulnerability Index",
       x = "Normalised Vulnerability Index",
       y = "Frequency")

# Plot spatial distribution
PLOT_rank <- merge(BRIS, DF_final, by = "SA1_CODE21")
ggplot() +
  geom_sf(data = PLOT_rank, 
          aes(fill = Vulnerability_Index_Rank_Normalised), 
          color = NA) +
  scale_fill_viridis_c(option = "rocket", direction = -1, begin = 0.2) +
  theme_void() +
  labs(fill = "Vulnerability Index (Ranked at final step)")

# Save the final vulnerability map
ggsave("../../visualisations/PCA/Vulnerability_Index_Ranked.png")

# ---------------------------------------------------------------------------- #