### ------------------------------------------------------------------------ ###
### ------------- CODE FOR CREATION OF VULNERABILITY INDEX SA1 ------------- ###
### ------------------------------------------------------------------------ ###


### Libraries: ------------------------------------------------------------- ###
### ------------------------------------------------------------------------ ###
library(sf)
library(tidyverse)
library(dplyr)
library(here)
library(data.table)
library(corrplot)
library(ggplot2)
library(tidyr)
library(corrr)
library(ggcorrplot)
library(factoextra)
library(gridExtra)

### SHAPEFILE LOADING: ----------------------------------------------------- ###
### ------------------------------------------------------------------------ ###
setwd("~/Flooding/flooding_vulnerability/scripts/4_vulnerability")
BRIS <- read_sf("../../data/spatial/SA1_Bris","bris_sa1")
DF <- st_drop_geometry(BRIS)[,1]
fPath <- "../../data/processed_data/"
fPath2 <- "../../data/data_products/"
# Task 1: Extract below and add to filepath - DONE
df <- read_sf("../../data/SA1_2021_AUST_SHP_GDA2020/SA1_2021_AUST_GDA2020.shp")

### NORMALISING FUNCTION: -------------------------------------------------- ###
### ------------------------------------------------------------------------ ###
normaliseData <- function(dummy){
  for (i in 2:ncol(dummy)){
    # Avoid modifying columns if they have a single unique value or are NA
    if(length(unique(dummy[, i])) > 1){
      dummy[, i] <- scale(dummy[, i])
    }
  }
  return(dummy)
}

### EXPOSURE (HIGH -> MORE VULNERABLE): ------------------------------------ ###
### ------------------------------------------------------------------------ ###
DF_E <- DF

# Inundation:
DATASET_E <- c("sa1_inundation")
for (i in 1:length(DATASET_E)){
  new <- read.csv(paste0(fPath2,DATASET_E[i],".csv"))
  new <- new[,c(1, ncol(new))]
  names(new)[1] <- "SA1_CODE21"
  DF_E <- merge(DF_E,new)
}

# Normalise instead of rank
DF_E <- normaliseData(DF_E)

if(ncol(DF_E) == 2){
  # Quick counter-measure while inundation is our only exposure parameter
  DF_E <- cbind(DF_E, "EXP" = DF_E[,2])
} 

# Does it look normally distributed? If not, apply a transform
hist(DF_E$EXP, breaks = 100) # Definitely right-skewed, so use log tform
DF_E$EXP <- log(DF_E$EXP + 1) # Adding 1 to avoid log(0)
hist(DF_E$EXP, breaks = 100) # Still right-skewed

# Plot Exposure (Revisit to adjust score representation)
PLOT <- merge(BRIS,DF_E)
ggplot() +
  geom_sf(data=PLOT, aes(fill = EXP), color="black") +
  theme_void() +
  # scale_fill_gradientn(colours = c("#C3DFCC", "#81C3D1",  "#579cc6", "#406ba8", "#2a3b8a", "#1b1f5d")) +
  scale_fill_distiller(direction = 1) +
  labs(fill="Exposure Score")


### SENSITIVITY (HIGH -> MORE VULNERABLE): --------------------------------- ###
### ------------------------------------------------------------------------ ###
DF_S <- DF

# Set sensitivity themes & variables within themes
# Task 2: Combine variables/themes so that themes don't have < 3 vars - DONE
S_THEMES <- list(
  # Socioeconomic Status (missing income status)
  soc_econ_stat = c("PopDensity2021",
                    "edu2021",
                    "unemployed2021"),
  # House and Household Composition
  house_comp = c("vulnerable_young2021",
                 "vulnerable_old2021",
                 "disabl2021",
                 "dwell_house2021",
                 "familydwelling2021"),
  # Health Status, Language, and Culture
  health_culture = c("esl2021", 
                    "indigenous2021",
                    "long-term_health_cond2021")
)

# ABS Demographics:
DATASET_S <- c("disabl2021",
               "dwell_house2021",
               "edu2021",
               "vulnerable_young2021",
               "vulnerable_old2021",
               "unemployed2021",
               "esl2021",
               "indigenous2021",
               "long-term_health_cond2021",
               "Population2021",
               "familydwelling2021")

# Read the files for each of the sensitivity elements
for (i in 1:length(DATASET_S)){
  new <- read.csv(paste0(fPath, "SA1/", DATASET_S[i], ".csv"))
  new <- new[, c(1, ncol(new))]
  names(new) <- c("SA1_CODE21", DATASET_S[i])
  DF_S <- merge(DF_S, new)
}    

# Task 3: Calculate population density = pop/area or people per km2 - DONE
merged_DF_S <- merge(df[, c("SA1_CODE21", "AREASQKM21")], 
                     DF_S, 
                     by = "SA1_CODE21", 
                     all.y = TRUE)
DF_S <- DF_S %>%
  mutate(PopDensity2021 = merged_DF_S$Population2021/merged_DF_S$AREASQKM21) %>%
  select(-Population2021) 

DF_S_long <- pivot_longer(DF_S, cols = -1)
hist(DF_S$vulnerable_old2021)
ggplot(filter(DF_S_long, name == "vulnerable_old2021",
              value > 0), aes(x = value)) + 
  geom_histogram(bins = 75) +
  facet_wrap(~name, scales = "free") +
  theme_minimal() +
  labs(x = "Value", y = "Frequency")
summary(DF_S$vulnerable_old2021)

test <- merge(BRIS, filter(DF_S_long, name == "vulnerable_old2021",
                           value < 1))
ggplot(test) +
  geom_sf(color = NA, aes(fill = value)) +
  xlim(c(152.8,153)) +
  ylim(c(-28,-27))

# Set any outliers more than 2 std from mean to min or max depending
for(column in 2:ncol(DF_S)){
  # Determine outliers
  lower_bound <- quantile(DF_S[, column], 0.025, na.rm = T)
  upper_bound <- quantile(DF_S[, column], 0.975, na.rm = T)
  outlier_ind_low <- which(DF_S[, column] < lower_bound)
  outlier_ind_high <- which(DF_S[, column] > upper_bound)
  
  # Set any outliers more than 2 std from mean to min or max depending
  if (length(outlier_ind_low) > 0) {
    DF_S[outlier_ind_low, column] <- min(DF_S[-outlier_ind_low, column], 
                                         na.rm = T)
  }
  
  if (length(outlier_ind_high) > 0) {
    DF_S[outlier_ind_high, column] <- max(DF_S[-outlier_ind_high, column], 
                                          na.rm = T)
  }
}

# Normalise the data
DF_S_colnames <- colnames(DF_S)
DF_S <- normaliseData(DF_S)
colnames(DF_S) <- DF_S_colnames

# Produce correlation plot
corr_matrix <- cor(DF_S[, 2:ncol(DF_S)],
                   use = "na.or.complete")
ggcorrplot(corr_matrix)

# PCA overall
data.pca <- princomp(corr_matrix)
summary(data.pca)
fviz_eig(data.pca, addlabels = TRUE) #scree

# Plot histograms
DF_S_long <- pivot_longer(DF_S, cols = -1)
ggplot(DF_S_long, aes(x = value)) + 
  geom_histogram(bins = 75) +
  facet_wrap(~name, scales = "free") +
  theme_minimal() +
  labs(x = "Value", y = "Frequency")
summary(DF_S)

# PCA for each theme
# Initialise lists to store results
s_theme_results <- list()
for (s_theme_name in names(S_THEMES)) {
  # Initialise a list for this theme
  results <- list()
  
  s_theme <- S_THEMES[[s_theme_name]]
  
  if (length(s_theme) > 1) {
    
    # Correlation plot of the theme
    theme_corr_matrix <- cor(DF_S[, s_theme],
                             use = "na.or.complete")
    ggcorrplot(theme_corr_matrix)
    results$corr_matrix <- theme_corr_matrix
    results$corr_plot <- ggcorrplot(theme_corr_matrix)
    
    # PCA of the theme
    theme_data.pca <- princomp(theme_corr_matrix)
    results$pca_summary <- summary(theme_data.pca)
    results$pca_loadings <- theme_data.pca$loadings
    
    # Scree plot
    results$scree_plot <- fviz_eig(theme_data.pca, addlabels = TRUE)
    
    # PCA variables plot
    results$pca_var_plot <- fviz_pca_var(theme_data.pca, col.var = "black")
  }
  
  # Store all results for this theme in the main list
  s_theme_results[[s_theme_name]] <- results
}
s_theme_results$soc_econ_stat

s_theme_results

# Task 4: For Comp columns in pca_loadings that sum below 0, multiply by -1
# Iterate over each theme's PCA results
for (s_theme_name in names(s_theme_results)) {
  if (!is.null(s_theme_results[[s_theme_name]]$pca_loadings)) {
    # Extract the loadings
    loadings <- s_theme_results[[s_theme_name]]$pca_loadings
    
    # Check the sum of each component's loadings
    for (comp in 1:ncol(loadings)) {
      if (sum(loadings[, comp]) < 0) {
        # Multiply the loadings by -1 if the sum is below 0
        loadings[, comp] <- loadings[, comp] * -1
      }
    }
    
    # Update the loadings in the results
    s_theme_results[[s_theme_name]]$pca_loadings <- loadings
    
    # Print adjusted loadings for verification
    print(paste("Adjusted loadings for theme:", s_theme_name))
    print(s_theme_results[[s_theme_name]]$pca_loadings)
  }
}

# Task 5: Multiply PC1 % of explained variance by Comp columns that are 
# useful (85% var explained) and sum to get overall index weights
# Function to calculate the overall index weights
calculate_index_weights <- function(pca_results) {
  # Get the explained variance
  explained_variance <- pca_results$sdev^2 / sum(pca_results$sdev^2)
  
  # Determine the number of components explaining at least 85% variance
  cumulative_variance <- cumsum(explained_variance)
  useful_components <- which(cumulative_variance <= 0.85)
  
  # Ensure at least one component is selected if none are below 85%
  if (length(useful_components) == 0) {
    useful_components <- 1
  }
  
  # Get the loadings of the useful components
  loadings <- pca_results$loadings[, useful_components, drop = FALSE]
  
  # Multiply the PC1 percentage of explained variance by the loadings
  pc1_variance <- explained_variance[1]
  index_weights <- rowSums(loadings * pc1_variance)
  
  return(index_weights)
}

# Iterate over each theme's PCA results to calculate index weights
for (s_theme_name in names(s_theme_results)) {
  if (!is.null(s_theme_results[[s_theme_name]]$pca_summary)) {
    pca_results <- s_theme_results[[s_theme_name]]$pca_summary
    index_weights <- calculate_index_weights(pca_results)
    s_theme_results[[s_theme_name]]$index_weights <- index_weights
  }
}

# Print the index weights for verification
for (s_theme_name in names(s_theme_results)) {
  if (!is.null(s_theme_results[[s_theme_name]]$index_weights)) {
    print(paste("Index weights for theme:", s_theme_name))
    print(s_theme_results[[s_theme_name]]$index_weights)
  }
}

# Task 6: View the weights
# Function to calculate and plot the index weights
plot_index_weights <- function(theme_name, index_weights) {
  # Convert the index weights to a dataframe for plotting
  weights_df <- data.frame(
    Variable = names(index_weights),
    Weight = index_weights
  )
  
  # Create a bar plot of the index weights
  ggplot(weights_df, 
         aes(x = reorder(Variable, Weight), y = Weight)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    coord_flip() + # Flip coordinates for better readability
    theme_minimal() +
    labs(
      title = paste("Index Weights for Theme:", 
                    theme_name),
      x = "Variable",
      y = "Weight"
    )
}

# Iterate over each theme's index weights and plot them
for (s_theme_name in names(s_theme_results)) {
  if (!is.null(s_theme_results[[s_theme_name]]$index_weights)) {
    index_weights <- s_theme_results[[s_theme_name]]$index_weights
    print(plot_index_weights(s_theme_name, 
                             index_weights))
  }
}

# Task 7: PCA within themes
# Function to impute missing values with column mean
impute_missing_values <- function(data) {
  data %>% mutate(across(everything(), 
                         ~ ifelse(is.na(.), 
                                  mean(., na.rm = TRUE), 
                                  .)))
}

# Perform PCA within each theme and store the PC1 scores
for (s_theme_name in names(S_THEMES)) {
  s_theme <- S_THEMES[[s_theme_name]]
  
  if (length(s_theme) > 1) {
    theme_data <- DF_S[, s_theme, drop = FALSE]
    theme_data <- impute_missing_values(theme_data) # Impute missing values
    theme_data_normalised <- normaliseData(theme_data) # Normalise data
    
    # Perform PCA
    pca_theme <- princomp(theme_data_normalised, 
                          cor = TRUE)
    
    # Store PC1 scores
    DF_S[[paste0(s_theme_name, "_PC1")]] <- pca_theme$scores[, 1]
    
    # Store results for reference
    s_theme_results[[s_theme_name]]$pca_scores <- pca_theme$scores[, 1]
    s_theme_results[[s_theme_name]]$pca_summary <- summary(pca_theme)
    s_theme_results[[s_theme_name]]$pca_loadings <- pca_theme$loadings
    s_theme_results[[s_theme_name]]$scree_plot <- fviz_eig(pca_theme, 
                                                           addlabels = TRUE)
    s_theme_results[[s_theme_name]]$pca_var_plot <- fviz_pca_var(pca_theme,
                                                                 col.var = "black")
  }
}

# Collect all the pca_var_plots into a list
pca_var_plots <- lapply(names(S_THEMES), function(s_theme_name) {
  if (!is.null(s_theme_results[[s_theme_name]]$pca_var_plot)) {
    return(s_theme_results[[s_theme_name]]$pca_var_plot)
  } else {
    return(NULL)
  }
})

# Remove NULL values from the list
pca_var_plots <- pca_var_plots[!sapply(pca_var_plots, is.null)]

# Arrange all PCA variable plots side by side
do.call("grid.arrange", c(pca_var_plots, ncol = length(pca_var_plots)))

# Task 8: PCA across themes
# Combine the PC1 scores from each theme into a new dataframe
DF_combined_themes <- DF_S %>%
  select(SA1_CODE21, ends_with("_PC1")) %>%
  drop_na() # Ensure there are no NA values

# Normalise the combined PC1 scores
DF_combined_themes_normalised <- normaliseData(DF_combined_themes)

# Perform PCA on the normalised combined PC1 scores (excluding SA1_CODE21)
pca_combined_themes <- princomp(DF_combined_themes_normalised[, -1], cor = TRUE)

# Summary of the PCA
summary(pca_combined_themes)

# Scree plot to visualise explained variance
fviz_eig(pca_combined_themes, addlabels = TRUE)

# Plot the PCA results
fviz_pca_var(pca_combined_themes, col.var = "black")

# Correlation plot of the combined PC1 scores
corr_matrix_combined_themes <- cor(DF_combined_themes_normalised[, -1], 
                                   use = "complete.obs")
ggcorrplot(corr_matrix_combined_themes)

# Print the loadings for interpretation
print(pca_combined_themes$loadings)

# Store PCA results in the list
s_theme_results[["combined_themes"]] <- list(
  pca_summary = summary(pca_combined_themes),
  pca_loadings = pca_combined_themes$loadings,
  scree_plot = fviz_eig(pca_combined_themes, addlabels = TRUE),
  pca_var_plot = fviz_pca_var(pca_combined_themes, col.var = "black"),
  corr_plot = ggcorrplot(corr_matrix_combined_themes)
)

# Task 9: Calculate overall sensitivity of each area from the theme weights
# Combine theme PC1 scores into a new dataframe
DF_sensitivity <- DF_S %>%
  select(SA1_CODE21, ends_with("_PC1"))

# Check if all themes have PC1 scores
missing_themes <- setdiff(names(S_THEMES), 
                          gsub("_PC1", "", 
                               colnames(DF_sensitivity)[-1]))
if (length(missing_themes) > 0) {
  stop(paste("Missing PC1 scores for themes:", 
             paste(missing_themes, 
                   collapse = ", ")))
}

# Calculate overall sensitivity score
DF_sensitivity <- DF_sensitivity %>%
  rowwise() %>%
  mutate(Overall_Sensitivity = sum(c_across(ends_with("_PC1")))) %>%
  ungroup()

# Normalise the overall sensitivity score to the range [0, 1]
min_sensitivity <- min(DF_sensitivity$Overall_Sensitivity)
max_sensitivity <- max(DF_sensitivity$Overall_Sensitivity)
DF_sensitivity <- DF_sensitivity %>%
  mutate(Normalised_Sensitivity = (Overall_Sensitivity - min_sensitivity) / 
           (max_sensitivity - min_sensitivity))

# View the resulting dataframe
head(DF_sensitivity)

# Plot normalised overall sensitivity
ggplot(DF_sensitivity, 
       aes(x = Normalised_Sensitivity)) +
  geom_histogram(fill = "steelblue") +
  theme_minimal() +
  labs(
    title = "Distribution of Normalised Sensitivity",
    x = "Normalised Sensitivity Score",
    y = "Frequency"
  ) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Plot
PLOT <- merge(BRIS, 
              DF_sensitivity)
ggplot() +
  geom_sf(data=PLOT, 
          aes(fill = Normalised_Sensitivity), 
          color="black") +
  scale_fill_distiller(palette = "YlOrBr", direction = 1) +
  theme_void() +
  labs(fill="Sensitivity (%)")

################################# UP TO HERE ###################################
### ADAPTIVE CAPACITY (HIGH -> MORE VULNERABLE): --------------------------- ###
### ------------------------------------------------------------------------ ###
DF_A <- DF

# Distance to services:
DATASET_A <- c("dAmbu",
               "dBuss",
               "dFire",
               "dHosp",
               "dPets",
               "dPoli",
               "dSesf",
               "dShop")

# Read the files for each of the sensitivity elements
for (i in 1:length(DATASET_A)){
  new <- read.csv(paste0(fPath,"SA1/",DATASET_A[i],".csv"))[,-1]
  DF_A <- merge(DF_A,new)
}

# Normalise the data
DF_A_colnames <- colnames(DF_A)
DF_A <- normaliseData(DF_A)
colnames(DF_A) <- DF_A_colnames

# Plot histograms
DF_A_long <- pivot_longer(DF_A, cols = -1)
ggplot(DF_A_long, aes(x = value)) + 
  geom_histogram(bins = 75) +
  facet_wrap(~name, scales = "free") +
  theme_minimal() +
  labs(x = "Value", y = "Frequency")
summary(DF_A)

# Set Adaptive Capacity themes & variables within themes
A_THEMES <- list(
  # Emergency Services
  emerg_service = c("dAmbu",
                    "dFire",
                    "dPoli",
                    "dSesf"),
  # Health Services
  health_service = c("dHosp"),
  # Necessities?
  necessities = c("dBuss",
                  "dPets", 
                  "dShop")
)

# (Change to) PCA for each theme
for (theme in A_THEMES) {
  for(element in theme) {
    # Get the column index
    element_indx <- which(colnames(DF_A) == element)
    
    
  }
  # PCA of the theme
  
}

# (Change to) PCA across themes

PLOT <- merge(BRIS,DF_A)
ggplot() +
  geom_sf(data=PLOT, aes(fill = AC), color="black") +
  # scale_fill_gradientn(colours = rev(c("#eeb6a3", "#df7890",  "#bd4c8a", "#7e2d91", "#52157f", "#381754"))) +
  scale_fill_distiller(palette = 'YlGn') +
  theme_void() +
  labs(fill="Adaptive Capacity (%)")


### VULNERABILITY (HIGH -> MORE VULNERABLE): ------------------------------- ###
### ------------------------------------------------------------------------ ###

DF_V <- merge(DF_E,DF_S) %>% merge(DF_A)
dummy <- cbind(DF_V$EXP,DF_V$SEN,DF_V$AC)
DF_V <- cbind(DF_V, "VI" = frank(rowSums(dummy), 
                                 na.last = "keep", 
                                 ties.method = c("min"))/nrow(dummy))

PLOT <- merge(BRIS,DF_V)
ggplot() +
  geom_sf(data=PLOT, aes(fill = VI), color="black") +
  scale_fill_viridis_c(option = "rocket", direction = -1, begin = 0.2) +
  # scale_fill_distiller(palette = 'BuPu', direction = 1) +
  theme_void() +
  labs(fill="Vulnerability (%)")

# Correlation plot:
corrplot(cor(DF_V[,2:ncol(DF_V)],use="complete.obs"))



# ---------------------------------------------------------------------------- #